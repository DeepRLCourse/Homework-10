{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "    <img src=\"https://www.sharif.ir/documents/20124/0/logo-fa-IR.png/4d9b72bc-494b-ed5a-d3bb-e7dfd319aec8?t=1609608338755\" alt=\"Logo\" width=\"200\">\n",
        "    <p><b> Reinforcement Learning Course, Dr. Rohban</b></p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Full Name:*\n",
        "\n",
        "*Student Number:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNM0OHRNjy62"
      },
      "source": [
        "\n",
        "# Random Network Distillation \n",
        "\n",
        "## Overview\n",
        "\n",
        "RND (Random Network Distillation) was first proposed in [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894), which introduces an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The exploration bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. RND claims that it is the first method that achieves better than average human performance on Montezuma’s Revenge without using demonstrations or having access to the underlying state of the game.\n",
        "\n",
        "## Quick Facts\n",
        "\n",
        "1. The insight behind exploration approaches is that we first establish a method to measure the **novelty of states**, namely, how well we know this state, or the number of times we have visited a state similar to it. Then we assign an exploration reward in proportional to the novelty measure of the state. If the visited state is more novel, or say the state is explored very few times, the agent will get a bigger intrinsic reward. On the contrary, if the agent is more familiar with this state, or say, the state has been explored many times, the agent will get a smaller intrinsic reward on this state.\n",
        "\n",
        "2. RND is a **prediction-error-based** exploration approach that can be applied in non-tabular cases. The main idea of prediction-error-based approaches is that defining the intrinsic reward as the prediction error for a problem related to the agent’s transitions, such as learning forward dynamics model, learning inverse dynamics model, or even a randomly generated problem, which is the case in RND algorithm.\n",
        "\n",
        "3. RND involves **two neural networks**: a fixed and randomly initialized target network which sets the prediction problem, and a predictor network trained on data collected by the agent.\n",
        "\n",
        "4. In RND paper, the underlying base RL algorithm is off-policy PPO. Generally, RND intrinsic reward generation model can be combined with many different RL algorithms such as DDPG, TD3, **SAC** conveniently.\n",
        "\n",
        "## Key Equations or Key Graphs\n",
        "\n",
        "The following two graphs are from OpenAI’s blog. The overall sketch of RND is as follows:\n",
        "\n",
        "### Random Network Distillation\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"https://opendilab.github.io/DI-engine/_images/rnd.png\" \n",
        "         alt=\"RND Architecture Diagram\" \n",
        "         style=\"max-width: 70%; border: 1px solid #ddd; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
        "    <p style=\"font-style: italic; color: #666;\">Random Network Distillation (RND) Architecture</p>\n",
        "</div>\n",
        "\n",
        "The overall sketch of next_sate_prediction exploration method is as follows:\n",
        "\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"https://opendilab.github.io/DI-engine/_images/rnd.png\" \n",
        "         alt=\"RND Architecture Diagram\" \n",
        "         style=\"max-width: 70%; border: 1px solid #ddd; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
        "    <p style=\"font-style: italic; color: #666;\">Random Network Distillation (RND) Architecture</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Prediction Error Factors in RND\n",
        "\n",
        "In the RND paper, the authors point out that prediction errors can be attributed to the following factors:\n",
        "\n",
        "1. **Amount of training data**  \n",
        "   Prediction error is high where few similar examples were seen by the predictor.\n",
        "\n",
        "2. **Stochasticity**  \n",
        "   Prediction error is high because the target function is stochastic. Stochastic transitions are a source of such error for forward dynamics prediction.\n",
        "\n",
        "3. **Model misspecification**  \n",
        "   Prediction error is high because information necessary for the prediction is missing, or the model class of predictors is too limited to fit the complexity of the target function.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Implementation Details That Matter\n",
        "\n",
        "### 1. Intrinsic Reward Normalization and Weight Factors\n",
        "- **Normalization Method**: Min-max normalization  \n",
        "  `normalized_reward = (reward - batch_min) / (batch_max - batch_min)`  \n",
        "  (Scales intrinsic reward to [0,1] range)\n",
        "  \n",
        "- **Weight Factors**:\n",
        "  - For MiniGrid: Last non-zero positive reward × 1000\n",
        "  - General case: Can use max game length as weight factor\n",
        "  - Critical for balancing exploration vs exploitation\n",
        "  - Experiments show proper weighting is essential for good performance in MiniGrid environments\n",
        "\n",
        "### 2. Observation Normalization\n",
        "- **Process**:\n",
        "  1. Subtract running mean\n",
        "  2. Divide by running standard deviation\n",
        "  3. Clip values to [-5, 5] range\n",
        "- **Initialization**:\n",
        "  - Use random agent to collect normalization parameters\n",
        "  - Small number of steps before training begins\n",
        "- **Application**:\n",
        "  - Same normalization for predictor and target networks\n",
        "  - Different normalization for policy network\n",
        "\n",
        "### 3. Non-Episodic Intrinsic Reward and Two Value Heads\n",
        "- **Non-Episodic Setting**:\n",
        "  - Returns continue across episodes (\"game over\" doesn't reset)\n",
        "  - Leads to more exploration without extrinsic rewards\n",
        "- **Dual Value Heads**:\n",
        "  - Recommended for combining episodic/non-episodic rewards\n",
        "- **Discount Factors**:\n",
        "  - Extrinsic rewards: γ = 0.999 (higher for better performance)\n",
        "  - Intrinsic rewards: γ = 0.99 (lower to maintain exploration)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup Code\n",
        "Before getting started we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n",
        "\n",
        "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### In the following cell you are going to direct to your gooledrive if you are using GooleColab which is preferable "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY7wnz1YGS5z"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# . Moount Google Drive\n",
        "# ----------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Go the Project directory\n",
        "# ----------------------------\n",
        "import os\n",
        "\n",
        "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
        "# Example: If you create a 2020FA folder and put all the files under A1 folder, then '2020FA/A1'\n",
        "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2020FA/A1'\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA-iCbPSj5rW"
      },
      "source": [
        "# ----------------------------\n",
        "# 2. Install dependencies\n",
        "# ----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj6IeGRij9D2"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf6_EjArkAz3"
      },
      "source": [
        "# ----------------------------\n",
        "# 3. Introduction\n",
        "# ----------------------------\n",
        "Welcome to the Random Network Distillation (RND) + PPO Homework!\n",
        "\n",
        "In this assignment, you will:\n",
        "- Understand and implement parts of the RND-based exploration method.\n",
        "- Train an agent using PPO + RND in the MiniGrid environment.\n",
        "- Analyze learning curves and evaluate agent performance.\n",
        "\n",
        "Modules to implement:\n",
        "- Complete missing parts in `Brain/brain.py` (intrinsic reward, RND loss).\n",
        "- Complete missing parts in `Brain/model.py` (TargetModel, PredictorModel).\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Student Instructions\n",
        "# ----------------------------\n",
        "> Please open and edit the following files:\n",
        "- `Brain/brain.py`\n",
        "- `Brain/model.py`\n",
        "\n",
        "> Specifically, look for `TODO` markers in the code and complete the necessary parts.\n",
        "\n",
        "After you have filled in the missing parts, you can proceed to train the agent.\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Train the Agent\n",
        "# ----------------------------\n",
        "\n",
        "Now that you've completed the TODOs, let's train your agent!\n",
        "This will launch the main script with training from scratch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42IgzWrukAjM"
      },
      "outputs": [],
      "source": [
        "!python main.py --train_from_scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As19xqS3kZP4"
      },
      "source": [
        "# ----------------------------\n",
        "# 6. Visualize Logs\n",
        "# ----------------------------\n",
        "launch TensorBoard to monitor your training logs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riwgtJAUkh56"
      },
      "outputs": [],
      "source": [
        "# Start Tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir Logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nBRTZdHkknG"
      },
      "source": [
        "# --------------------------------------------------\n",
        "# End of Starter Notebook\n",
        "# --------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
